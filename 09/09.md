# 파이썬 - 스타일 전이

- 스타일 전이: Style Transfer
  - 주로 그래픽 디자인 분야에서 사용
  - 이미지의 스타일을 다른 이미지로 전이하는 기술
- 컴퓨터비전에서의 스타일 전이
  - 주로 이미지 **생성 모델(Generative Model)** 이나 **생산적 적대 신경망 모델(Generative Adversarial Network: GAN)** 을 활용해 구현  
    :arrow_right: 신경망 기반
- 가장 널리 사용되는 스타일 전이 모델 중 하나는 **신경망 스타일 전이(Neural Style Transfer: NST)**
  - 이미지의 내용을 유지하면서 다른 이미지의 스타일을 적용한 결과물 생성
- 기존 이미지의 내용을 나타내는 특징 맵과 다른 이미지의 스타일을 나타내는 특징 맵을 결합해 새로운 이미지를 생성함
- **콘텐츠 이미지(Content Image)** 와 **스타일 이미지(Style Image)** 의 결합으로 이미지 생성
  - 콘텐츠 이미지: 생성하려는 최종 이미지의 내용을 의미
  - 스타일 이미지: 적용하고 싶은 스타일을 가진 이미지를 의미
  - 출력 이미지: 콘텐츠 이미지의 내용과 스타일 이미지의 스타일을 결합해 만듦
- 신경망에 콘텐츠 이미지를 통과시켜 각 계층에서의 이미지의 특징을 추출함
  - 각 계층마다 추상적인 정보를 갖게 되는데, 이때 이미지 내용에 관한 특징들 추출
- 스타일 이미지도 신경망을 통과해 각 계층에서의 특징을 얻게 됨
  - 스타일을 나타내기 위해 각 계층에서의 특징 상관 관계를 분석함
  - 이 두 특징을 활용해 스타일 이미지의 스타일을 최대한 반영하는 생성 이미지를 찾는 것을 목표로 해 모델이 생성됨
- 다음 과정을 반복하면서 생성 이미지는 콘텐츠 이미지의 내용과 스타일 이미지의 스타일을 동시에 반영하게 됨
  - 콘텐츠 이미지와 생성 이미지 간의 차이를 계산해 콘텐츠 일치 측정
  - 스타일 이미지와 생성 이미지 간의 특징 상관 관계의 차이를 계산해 스타일의 일치 측정

## 01 인물 세그멘테이션(PP-HumanSeg)

- PP 인물 세그멘테이션
  - 패들세그(PaddleSeg) 기반의 모델
  - 이미지나 비디오에서 인물의 영역을 정확히 분리하고 식별하는 데 사용됨
  - 이 모델은 뛰어난 정확도와 높은 추론 속도를 제공해 효율적인 활용 가능

```python
import cv2
import numpy as np


def segmentation(model_path, image):
    height, width = image.shape[:2]
    model = cv2.dnn.readNetFromONNX(model_path)

    image = image.astype(np.float32, copy=False) / 255.0
    image -= np.full_like(image, [0.5, 0.5, 0.5], dtype=np.float32)
    image /= np.full_like(image, [0.5, 0.5, 0.5], dtype=np.float32)

    input_blob = cv2.dnn.blobFromImage(image, 1.0, (192, 192), swapRB=True)
    model.setInput(input_blob)

    blob_name = model.getUnconnectedOutLayersNames()
    output_blob = model.forward(blob_name[0])

    print(blob_name)
    print(output_blob.shape)
    print(output_blob.min(), output_blob.max())


if __name__ == "__main__":
    src = cv2.imread("image.jpg")
    seg = segmentation("./models/onnx_model/human_segmentation.onnx", src)
```

- PP 인물 세그멘테이션 모델이 학습한 데이터세트
  - RGB 형식의 부동 소수점 이미지로 이루어짐
  - 평균 0.5, 표준 편차 0.5로 정규화됨
  - 데이터의 픽셀 값들을 0에서 1 사이의 범위로 조정해 모델이 더 빠르게 수렴하고 안정적으로 학습할 수 있도록 구성됨
- 입력 이미지 전처리
  - 부호 없는 8비트 정수(`uint8`) 이미지이므로 부동 소수점 이미지(`float32`)로 변환, `255`로 나누어 0에서 1 사이의 범위로 정규화
  - 이미지를 `0.5`의 값으로 감산해 모델이 학습할 때 수행한 **중심화(Centering)** 과정과 동일한 값 적용 후 `0.5`의 표준 편차로 정규화
  - 모델이 학습 데이터와 일관된 형식으로 입력 이미지를 처리할 수 있도록 데이터를 조정하는 작업
- PP 인물 세그멘테이션 모델은 192×192 크기의 RGB 이미지로 학습됨
  - 동일한 전처리 과정을 블롭 이미지 생성 시에도 적용
- 생성된 블롭 이미지는 네트워크 입력 메서드에 전달
- 출력층은 `save_infer_model/scale_0.tmp_1`로 하나의 계층만 존재
  - 튜플 형태로 생성되지 않기 위해 순전파 수행 시 첫 번째 색인 값만 전달해 사용
  - 순전파 메서드에 `blob_name`으로 입력하면 `((1, 2, 192, 192), )`와 같이 튜플 형식으로 반환됨
- 출력 결과를 보면 PP 인물 세그멘테이션 모델은 (배치 크기, 채널, 이미지 높이, 이미지 너비)로 반환됨
  - 0에서 1 사이의 값으로 반환

```python
def segmentation(model_path, image):
    ...

    dst = output_blob[0].transpose(1, 2, 0)
    dst = cv2.resize(dst, (width, height), interpolation=cv2.INTER_LINEAR)

    dst = dst.transpose(2, 0, 1)[np.newaxis, ...]
    dst = np.argmax(dst, axis=1)[0, :, :]

    dst = (dst * 255).astype(np.uint8)[..., np.newaxis]

    return dst


if __name__ == "__main__":
    src = cv2.imread("image.jpg")
    seg = segmentation("./models/onnx_model/human_segmentation.onnx", src)

    cv2.imshow("segmentation", seg)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
```

- 마스크 이미지로 변환하는 예
- `output_blob`
  - (배치 크기, 채널, 이미지 높이, 이미지 너비)의 차원을 가짐
  - 첫 번째 인덱스만 가져와 (이미지 높이, 이미지 너비, 채널)로 전치
  - 이후 192×192 크기의 이미지를 입력 이미지와 동일한 크기로 조정
- 다시 출력 결과를 (채널, 이미지 높이, 이미지 너비) 구조로 변경
- 축을 추가해 (1, 채널, 이미지 높이, 이미지 너비)로 변경  
  :arrow_right: 출력 결과가 두 개의 채널로 세그멘테이션 결과를 반환하므로 두 채널 중 가장 높은 값을 가진 값의 색인 반환
- 마스크
  - 0: 사람으로 간주되지 않은 마스크
  - 1: 사람으로 간주된 마스크
- 더 큰 값으로 예측한 결과를 반영하기 위해 `argmax` 함수로 더 높은 색인 값을 갖는 마스크 생성
- `argmax` 함수를 통과하면 (1, 이미지 높이, 이미지 너비)가 되므로 다시 전치해 (이미지 높이, 이미지 너비, 1)의 형태로 변경

## 02 신경망 스타일 전이(Fast Neural Style)

- 이번 장에서 사용되는 스타일 전이 모델은 **빠른 신경망 전이(Fast Neural Style)** 모델
  - 기존의 스타일 전이 모델의 속도 문제를 극복하고 빠른 속도로 스타일을 변환하기 위해...
    - 다양한 최적화 기법과 경량화된 구조 사용
  - 이미지에 적용되는 스타일을 효과적으로 추출하면서도 연산 속도를 크게 향상시킨 모델
- 빠른 신경망 전이 모델의 세 단계
    1. 이미지 다운샘플링
    2. 레즈넷에서 사용하는 **잔차 블록(Residual Block)** 을 활용해 이미지의 특징 추출 및 보존
        - 합성곱 계층과 정규화 계층으로 이루어져 있어 입력을 받아 일부 특징을 추출하고 나머지를 보존해 모델 안정성 높임
    3. 업샘플링을 통해 이미지를 원래 크기로 복구
- 빠른 신경망 전이 모델은 총 네 가지의 사전 학습된 가중치를 제공함
  - 이러한 모델은 파이토치로 구현되어 있어 파이토치 라이브러리를 사용해야 함
  - OpenCV에서 사용할 수 있도록 ONNX 표준으로 변환해 봄
- ONNX: 다양한 딥러닝 프레임워크 간에 모델을 공유하기 위한 개방형 표준

## 03 파이토치 모델 변환

- 파이토치 깃허브의 `fast_neural_style/neural_style/transformer_net.py`
  - 빠른 스타일 전이 모델의 코드를 담고 있는 파일
  - 전체 모델 구조를 정의하고 구현

```python
import torch
from transformer_net import TransformerNet

for name in ["mosaic", "candy", "rain_princess", "udnie"]:
    model = TransformerNet()
    state_dict = torch.load(f"./models/pytorch_model/{name}.pth")

    for key in list(state_dict.keys()):
        if key.endswith(("running_mean", "running_var")):
            del state_dict[key]

    model.load_state_dict(state_dict)
    model.eval()

    dummy_input = torch.randn(1, 3, 256, 256)
    torch.onnx.export(model, dummy_input, f"./models/onnx_model/{name}.onnx")
```

- 모델을 변환하기 위해...
  - **빠른 스타일 전이 클래스(`TransformerNet`)** 초기화
  - **파이토치 모델 불러오기 함수(`torch.load`)** 로 사전 학습된 가중치를 불러옴
- `state_dict`에서 `running_mean`과 `running_var`로 끝나는 특정 가중치를 제거해야 함
  - 해당 가중치는 **배치 정규화(Batch Normalization)** 에 활용  
    - 딥러닝 모델의 각 계층의 입력을 정규화하여 학습을 안정화하고 속도를 향상시키는 기술
  - 모델 학습 중 미니배치에 대한 평균과 분산을 계산하는 데 사용됨
  - 이러한 통계 값은 학습 과정에서 사용되며, 추론 단계에서는 학습 중에 계산한 이동 평균을 활용함
  - 파이토치에서는 이러한 통계 정보가 모델의 `state_dict`에 저장되어 있음
  - ONNX 표준은 모델 구조와 매개 변수만 저장하므로 배치 정규화 계층에 대한 정보가 불필요
- 빠른 스타일 전이 모델에 가중치 입력
  - 모델이 이전에 학습한 정보로 변경 가능
  - 모델의 가중치를 모두 불러왔다면 모델을 **평가 모드(`eval`)** 로 변경
- 모델이 학습할 때 사용하는 기법 중 하나인 배치 정규화 등은 학습 중에만 활성화되어야 함
  - 이 기법들은 모델이 학습 데이터에 적응해 좋은 성능을 낼 수 있게 해줌
  - 추론이나 테스트 시에는 비활성화되어야 함
- ONNX로 모델을 변환할 때 평가 모드로 설정하면...
  - 배치 정규화와 같이 학습 중에만 적용되는 기법들이 비활성화되어 추론 과정에서 일관된 결과를 얻을 수 있음
  - 모델의 변환된 버전이 학습된 대로 작동함을 보장함
- **더미 입력(`dummy_input`)** 을 생성해 ONNX로 변환
  - 더미는 ONNX로 모델을 변환할 때 모델의 입력 형태를 명시적으로 지정하는 데 사용
  - ONNX는 모델의 입출력 형태를 알아야 하기 때문에 실제 데이터가 아닌 가상의 더미 입력을 전달해 모델의 입력 형태 정의
- 빠른 스타일 전이 모델은 256×256×3의 이미지로 사전 학습되었으므로, `(1, 3, 256, 256)`의 구조를 갖는 더미 생성
  - (배치 크기, 채널, 이미지 높이, 이미지 너비) 의미

## 04 스타일 적용

- 빠른 신경망 전이 모델은 입력 이미지 크기에 대해 유연성을 가짐
- 이 모델은 사전 학습된 모델로부터 학습된 가중치를 가져와 새로운 작업에 적용함
  - 다양한 크기의 이미지가 입력되더라도 모델이 이미 학습한 특징을 보존하고 스타일을 적용할 수 있음
- 다양한 입력 크기가 가능한 이유는 모델 구조와 사용된 네트워크 계층의 특성 때문
  - 합성곱 계층: Convolutional Layer
    - 입력 데이터에 대해 필터를 사용해 지역적인 패턴을 감지하고 학습하는 계층
    - 지역적인 특징을 공간적으로 유지하면서 학습할 수 있어 입력 이미지의 크기에 대해 상대적으로 민감하지 않음  
      :arrow_right: 입력 이미지의 크기가 달라지더라도 네트워크가 이미지의 특징을 인식할 수 있음
  - 풀링 계층: Pooling Layer
    - **적응형 풀링(Adaptive Pooling)** 을 사용해 입력 이미지의 크기에 상관없이 고정된 크기의 출력 생성
    - 모델이 입력 이미지의 크기에 상대적으로 불변하게 만들어 크기와 상관없이 동일한 스타일을 전달할 수 있게 함
    - 입력 이미지를 모델에 전달하기 전에 크기를 조절할 필요가 없게 됨
  - 인스턴스 정규화: Instance Normalization
    - 입력 이미지의 평균과 표준 편차를 계산하고 이를 사용해 정규화를 수행하는 계층
    - 채널을 기준으로 정규화를 수행하므로 각 샘플에 대해 개별적으로 수행됨
    - 입력이 다른 크기를 가져도 독립적으로 작동 가능
- 입력된 이미지 크기가 클수록 더 많은 스타일이 전이되어 더 큰 공간적인 특징을 감지하게 됨
  - 크기가 큰 이미지는 작은 이미지보다 더 많은 세부 사항과 다양한 구조를 포함, 더 넓은 컨텍스트를 이해하고 스타일 적용 가능

```python
import cv2
import numpy as np


...


def style_transfer(model_path, image):
    height, width = image.shape[:2]
    model = cv2.dnn.readNetFromONNX(model_path)

    input_blob = cv2.dnn.blobFromImage(image, swapRB=True)
    model.setInput(input_blob)

    blob_name = model.getUnconnectedOutLayersNames()
    output_blob = model.forward(blob_name[0])

    dst = output_blob[0].transpose((1, 2, 0))
    dst = dst.clip(0, 255).astype(np.uint8)

    dst = cv2.cvtColor(dst, cv2.COLOR_RGB2BGR)
    dst = cv2.resize(dst, (width, height), interpolation=cv2.INTER_LINEAR)
    return dst

if __name__ == "__main__":
    src = cv2.imread("image.jpg")
    stl = style_transfer("./models/onnx_model/mosaic.onnx", src)

    cv2.imshow("stl", stl)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
```

- 빠른 스타일 전이 모델 추론
- 이 모델이 학습된 이미지는 RGB 이미지이므로 **RGB 채널 변경(`swapRB`)** 만 참 값으로 할당
- 빠른 신경망 전이 모델도 `472`라는 하나의 계층만 존재
  - 튜플 형태로 생성되지 않기 위해 순전파 수행 시 첫 번째 색인 값만 전달해 사용
- `output_blob`
  - (채널, 이미지 높이, 이미지 너비)의 차원을 가짐
  - 전치 메서드를 활용해 (이미지 높이, 이미지 너비, 채널) 구조로 변경
- 빠른 신경망 전이 모델은 출력을 정규화하거나 특정 범위로 제한하는 계층을 사용하지 않음
  - `output_blob` 값이 음수이거나 255를 초과할 수 있음
  - 일반적으로 클립 메서드(`clip`)를 사용해 값 조정
    - 0 미만의 값은 0, 255 초과 값은 255로 제한
- 이미지를 다시 BGR 채널로 변경하고 본래의 크기로 재조절
  - 일반적으로 `output_blob`의 이미지 높이와 너비는 입력 이미지의 높이와 너비와 동일한 크기로 생성됨
  - 업샘플링 계층으로 출력 이미지의 크기가 늘어나거나 줄어들 수 있음
  - **이미지 크기 조절 함수(`cv2.resize`)** 로 크기를 다시 조절

```python
import cv2
import numpy as np


...


def image_transfer(src, segment, style):
    result1 = np.where(segment == 255, src, style)
    result2 = np.where(segment == 0, src, style)
    return result1, result2


if __name__ == "__main__":
    src = cv2.imread("image.jpg")
    seg = segmentation("./models/onnx_model/human_segmentation.onnx", src)
    stl = style_transfer("./models/onnx_model/mosaic.onnx", src)

    result1, result2 = image_transfer(src, seg, stl)
    result = cv2.hconcat((result1, result2))

    cv2.imshow("result", result)
    cv2.waitKey(0)
    cv2.destroyAllWindows()
```

- 스타일 병합 예
- **조건문 함수(`where`)** 활용
  - `np.where(조건문, 참일 때 값, 거짓일 때 값)`
  - 마스크 이미지 위에 스타일 전이 이미지를 쉽게 적용할 수 있음
- `result1`
  - 세그먼트 이미지가 인물이라면 해당 위치는 원본 이미지 픽셀 값 할당
  - 아니라면 스타일 전이 이미지 적용
- `result2`
  - 세그먼트 이미지가 배경이라면 원본 이미지의 픽셀 값 할당
  - 아니라면 스타일 전이 이미지 적용

## 스타일 전이 전체 코드

https://github.com/lemon-lime-honey/opencv-study/blob/5fea621b0f951d76e6610341647d3b58719ba10a/09/example/example1.py#L1-L64
